# 6.1.1文生视频原理

### CogVideo预训练模型

文生视频模型由于可训练的数据少，且视频持续时间差别很大，使用固定帧数的剪辑训练会破坏动作的连贯性，使模型混淆动作含义。

在这篇文章中，作者使用了多帧率分层训练法，提出了一个大规模的文生视频的预训练模型。(https://arxiv.org/abs/2205.15868)

#### 多帧率分层训练

![多帧率分层训练](/content/images/6.1.1multi_frame.png)

其原理可以简单理解为：

1、先根据语义生成关键视频帧的图像

2、基于关键帧的图像，生成连贯动作的补帧

也就是说，模型有两个部分需要训练，一个顺序的生成模型和一个递归的插值模型

由于帧的插值依赖于双向信息，所以模型应该同时有单向和双向区域。图中阶段一的所有帧和阶段二的2、4帧为单向区域，阶段二的其余帧属于双向区域。

#### 双通道注意力机制

![双通道注意力机制](/content/images/6.1.1dual_channel_attention.png)



CogVideo模型使用了CogView2文生图预训练模型的初始参数，这张图展示了其transformer layer层的结构，只有初始的参数是继承自CogView2，而后只有attention plus部分可训练。

包含Sandwich-LN的dual-channel注意力模块可计算为：


$$
\begin{aligned}
\widetilde{x}& =\alpha\cdot\mathrm{attention-base}(\mathrm{LayerNorm}(x_{in}))  \\
&+(1-\alpha)\cdot\text{attention-plus}(\text{LayerNorm}(x_{in})), \\
x_{out}& =x_{in}+\text{Layer}\mathrm{Norm}(\widetilde{x}). 
\end{aligned}
$$



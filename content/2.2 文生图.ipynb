{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. StableDiffusionPipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cc48b4ea5f0e5ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StableDiffusionPipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for text-to-image generation using Stable Diffusion.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
    "\n",
    "    Args:\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion uses the text portion of\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
    "            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
    "            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
    "        safety_checker ([`StableDiffusionSafetyChecker`]):\n",
    "            Classification module that estimates whether generated images could be considered offensive or harmful.\n",
    "            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n",
    "        feature_extractor ([`CLIPFeatureExtractor`]):\n",
    "            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
    "    \"\"\"\n",
    "    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModel,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: KarrasDiffusionSchedulers,\n",
    "        safety_checker: StableDiffusionSafetyChecker,\n",
    "        feature_extractor: CLIPFeatureExtractor,\n",
    "        requires_safety_checker: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n",
    "            deprecation_message = (\n",
    "                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n",
    "                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n",
    "                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n",
    "                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n",
    "                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n",
    "                \" file\"\n",
    "            )\n",
    "            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            new_config = dict(scheduler.config)\n",
    "            new_config[\"steps_offset\"] = 1\n",
    "            scheduler._internal_dict = FrozenDict(new_config)\n",
    "\n",
    "        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n",
    "            deprecation_message = (\n",
    "                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n",
    "                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n",
    "                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n",
    "                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n",
    "                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n",
    "            )\n",
    "            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            new_config = dict(scheduler.config)\n",
    "            new_config[\"clip_sample\"] = False\n",
    "            scheduler._internal_dict = FrozenDict(new_config)\n",
    "\n",
    "        if safety_checker is None and requires_safety_checker:\n",
    "            logger.warning(\n",
    "                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n",
    "                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n",
    "                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n",
    "                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n",
    "                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n",
    "                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n",
    "            )\n",
    "\n",
    "        if safety_checker is not None and feature_extractor is None:\n",
    "            raise ValueError(\n",
    "                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n",
    "                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n",
    "            )\n",
    "\n",
    "        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n",
    "            version.parse(unet.config._diffusers_version).base_version\n",
    "        ) < version.parse(\"0.9.0.dev0\")\n",
    "        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n",
    "        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n",
    "            deprecation_message = (\n",
    "                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n",
    "                \" 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the\"\n",
    "                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n",
    "                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n",
    "                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n",
    "                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n",
    "                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n",
    "                \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n",
    "                \" the `unet/config.json` file\"\n",
    "            )\n",
    "            deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "            new_config = dict(unet.config)\n",
    "            new_config[\"sample_size\"] = 64\n",
    "            unet._internal_dict = FrozenDict(new_config)\n",
    "\n",
    "        self.register_modules(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
    "        self.register_to_config(requires_safety_checker=requires_safety_checker)\n",
    "\n",
    "    def enable_vae_slicing(self):\n",
    "        r\"\"\"\n",
    "        Enable sliced VAE decoding.\n",
    "\n",
    "        When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n",
    "        steps. This is useful to save some memory and allow larger batch sizes.\n",
    "        \"\"\"\n",
    "        self.vae.enable_slicing()\n",
    "\n",
    "    def disable_vae_slicing(self):\n",
    "        r\"\"\"\n",
    "        Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n",
    "        computing decoding in one step.\n",
    "        \"\"\"\n",
    "        self.vae.disable_slicing()\n",
    "\n",
    "    def enable_vae_tiling(self):\n",
    "        r\"\"\"\n",
    "        Enable tiled VAE decoding.\n",
    "\n",
    "        When this option is enabled, the VAE will split the input tensor into tiles to compute decoding and encoding in\n",
    "        several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\n",
    "        \"\"\"\n",
    "        self.vae.enable_tiling()\n",
    "\n",
    "    def disable_vae_tiling(self):\n",
    "        r\"\"\"\n",
    "        Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n",
    "        computing decoding in one step.\n",
    "        \"\"\"\n",
    "        self.vae.disable_tiling()\n",
    "\n",
    "    def enable_sequential_cpu_offload(self, gpu_id=0):\n",
    "        r\"\"\"\n",
    "        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n",
    "        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n",
    "        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n",
    "        Note that offloading happens on a submodule basis. Memory savings are higher than with\n",
    "        `enable_model_cpu_offload`, but performance is lower.\n",
    "        \"\"\"\n",
    "        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.14.0\"):\n",
    "            from accelerate import cpu_offload\n",
    "        else:\n",
    "            raise ImportError(\"`enable_sequential_cpu_offload` requires `accelerate v0.14.0` or higher\")\n",
    "\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "\n",
    "        if self.device.type != \"cpu\":\n",
    "            self.to(\"cpu\", silence_dtype_warnings=True)\n",
    "            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n",
    "\n",
    "        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n",
    "            cpu_offload(cpu_offloaded_model, device)\n",
    "\n",
    "        if self.safety_checker is not None:\n",
    "            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)\n",
    "\n",
    "    def enable_model_cpu_offload(self, gpu_id=0):\n",
    "        r\"\"\"\n",
    "        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n",
    "        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n",
    "        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n",
    "        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n",
    "        \"\"\"\n",
    "        if is_accelerate_available() and is_accelerate_version(\">=\", \"0.17.0.dev0\"):\n",
    "            from accelerate import cpu_offload_with_hook\n",
    "        else:\n",
    "            raise ImportError(\"`enable_model_offload` requires `accelerate v0.17.0` or higher.\")\n",
    "\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "\n",
    "        if self.device.type != \"cpu\":\n",
    "            self.to(\"cpu\", silence_dtype_warnings=True)\n",
    "            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n",
    "\n",
    "        hook = None\n",
    "        for cpu_offloaded_model in [self.text_encoder, self.unet, self.vae]:\n",
    "            _, hook = cpu_offload_with_hook(cpu_offloaded_model, device, prev_module_hook=hook)\n",
    "\n",
    "        if self.safety_checker is not None:\n",
    "            _, hook = cpu_offload_with_hook(self.safety_checker, device, prev_module_hook=hook)\n",
    "\n",
    "        # We'll offload the last model manually.\n",
    "        self.final_offload_hook = hook\n",
    "\n",
    "    @property\n",
    "    def _execution_device(self):\n",
    "        r\"\"\"\n",
    "        Returns the device on which the pipeline's models will be executed. After calling\n",
    "        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n",
    "        hooks.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.unet, \"_hf_hook\"):\n",
    "            return self.device\n",
    "        for module in self.unet.modules():\n",
    "            if (\n",
    "                hasattr(module, \"_hf_hook\")\n",
    "                and hasattr(module._hf_hook, \"execution_device\")\n",
    "                and module._hf_hook.execution_device is not None\n",
    "            ):\n",
    "                return torch.device(module._hf_hook.execution_device)\n",
    "        return self.device\n",
    "\n",
    "    def _encode_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "        negative_prompt=None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "        Args:\n",
    "             prompt (`str` or `List[str]`, *optional*):\n",
    "                prompt to be encoded\n",
    "            device: (`torch.device`):\n",
    "                torch device\n",
    "            num_images_per_prompt (`int`):\n",
    "                number of images that should be generated per prompt\n",
    "            do_classifier_free_guidance (`bool`):\n",
    "                whether to use classifier free guidance or not\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n",
    "                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "        \"\"\"\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        if prompt_embeds is None:\n",
    "            text_inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "                text_input_ids, untruncated_ids\n",
    "            ):\n",
    "                removed_text = self.tokenizer.batch_decode(\n",
    "                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n",
    "                )\n",
    "                logger.warning(\n",
    "                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "                )\n",
    "\n",
    "            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n",
    "                attention_mask = text_inputs.attention_mask.to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            prompt_embeds = self.text_encoder(\n",
    "                text_input_ids.to(device),\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            prompt_embeds = prompt_embeds[0]\n",
    "\n",
    "        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
    "\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "            uncond_tokens: List[str]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"] * batch_size\n",
    "            elif type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "\n",
    "            max_length = prompt_embeds.shape[1]\n",
    "            uncond_input = self.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n",
    "                attention_mask = uncond_input.attention_mask.to(device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            negative_prompt_embeds = self.text_encoder(\n",
    "                uncond_input.input_ids.to(device),\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
    "\n",
    "            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "        return prompt_embeds\n",
    "\n",
    "    def run_safety_checker(self, image, device, dtype):\n",
    "        if self.safety_checker is not None:\n",
    "            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n",
    "            image, has_nsfw_concept = self.safety_checker(\n",
    "                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n",
    "            )\n",
    "        else:\n",
    "            has_nsfw_concept = None\n",
    "        return image, has_nsfw_concept\n",
    "\n",
    "    def decode_latents(self, latents):\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        return image\n",
    "\n",
    "    def prepare_extra_step_kwargs(self, generator, eta):\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "        # and should be between [0, 1]\n",
    "\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        # check if the scheduler accepts generator\n",
    "        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        if accepts_generator:\n",
    "            extra_step_kwargs[\"generator\"] = generator\n",
    "        return extra_step_kwargs\n",
    "\n",
    "    def check_inputs(\n",
    "        self,\n",
    "        prompt,\n",
    "        height,\n",
    "        width,\n",
    "        callback_steps,\n",
    "        negative_prompt=None,\n",
    "        prompt_embeds=None,\n",
    "        negative_prompt_embeds=None,\n",
    "    ):\n",
    "        if height % 8 != 0 or width % 8 != 0:\n",
    "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
    "\n",
    "        if (callback_steps is None) or (\n",
    "            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n",
    "                f\" {type(callback_steps)}.\"\n",
    "            )\n",
    "\n",
    "        if prompt is not None and prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
    "                \" only forward one of the two.\"\n",
    "            )\n",
    "        elif prompt is None and prompt_embeds is None:\n",
    "            raise ValueError(\n",
    "                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n",
    "            )\n",
    "        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        if negative_prompt is not None and negative_prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n",
    "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n",
    "            )\n",
    "\n",
    "        if prompt_embeds is not None and negative_prompt_embeds is not None:\n",
    "            if prompt_embeds.shape != negative_prompt_embeds.shape:\n",
    "                raise ValueError(\n",
    "                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n",
    "                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n",
    "                    f\" {negative_prompt_embeds.shape}.\"\n",
    "                )\n",
    "\n",
    "    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n",
    "        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        if latents is None:\n",
    "            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "        else:\n",
    "            latents = latents.to(device)\n",
    "\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @replace_example_docstring(EXAMPLE_DOC_STRING)\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n",
    "                instead.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
    "                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.\n",
    "                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
    "                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
    "                argument.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "            cross_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds\n",
    "        )\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        prompt_embeds = self._encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = self.unet.in_channels\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                ).sample\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        callback(i, t, latents)\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "        elif output_type == \"pil\":\n",
    "            # 8. Post-processing\n",
    "            image = self.decode_latents(latents)\n",
    "\n",
    "            # 9. Run safety checker\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "\n",
    "            # 10. Convert to PIL\n",
    "            image = self.numpy_to_pil(image)\n",
    "        else:\n",
    "            # 8. Post-processing\n",
    "            image = self.decode_latents(latents)\n",
    "\n",
    "            # 9. Run safety checker\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "\n",
    "        # Offload last model to CPU\n",
    "        if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
    "            self.final_offload_hook.offload()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ae90d1d5e854940a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> vae()\n",
    "> text_encoder()\n",
    "> unet()\n",
    "> schedule()\n",
    "> feature_extractor()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c93e89ff5be5cc4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO 上述方法的具体示例"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da21a165297e586"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. vae()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9145e817f4ab91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. text_encoder()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21fb6168a3b466d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. unet()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1df2fc12026164eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. schedule()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1afeafb19686c01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. feature_extractor()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20db3bcbc6982005"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Lora "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0441fbba068abf3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 通过OpenIVNO加速CPU出图速度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5f456186af8d51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7fc41efc03cfcafc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
